{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de Datos\n",
    "\n",
    "##### Futuras mejoras\n",
    "* Agregar la diferencia de tiempo en que se consiguió controlar el incendio\n",
    "* Analizar mejor de qué forma se distribuyen los datos nulos y de qué forma puedo rellenar los valores faltantes, si es posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rutas usuales que se ocuparán en el notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Salvar DataFrame?\n",
    "SAVE_DF = True\n",
    "\n",
    "# RUTAS\n",
    "MAIN_PATH = path.join(\"..\")\n",
    "\n",
    "DATA_PATH = path.join(MAIN_PATH, \"data\")\n",
    "WF_FOLDER_PATH = path.join(DATA_PATH, \"wildfires_us\")\n",
    "WF_DATA_PATH = path.join(WF_FOLDER_PATH, \"FPA_FOD_20170508.sqlite\")\n",
    "WF_DATA_COLUMNS_PATH = path.join(WF_FOLDER_PATH, \"COLUMNS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que se ocupará en la carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def crear_puntero(path=WF_DATA_PATH, msg=False):\n",
    "    \"\"\"Función que crea un puntero con el dataset de incendios forestales en US.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(path)\n",
    "        if msg:\n",
    "            print(\"Conexión realizada con éxito.\")\n",
    "    except:\n",
    "        if msg:\n",
    "            print(\"No se encuentra el archivo.\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que se ocupará para imprimir la información (número de filas y columnas) de un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def print_cantidad(dataframe):\n",
    "    \"\"\"Imprime la cantidad de datos que tiene el Data Frame.\n",
    "    \"\"\"\n",
    "    msg_cantidad = \"El dataset tiene una cantidad de {} datos y {} variables.\"\n",
    "    print(msg_cantidad.format(dataframe.shape[0], dataframe.shape[1]))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columnas a ocupar\n",
    "\n",
    "Se escojen las columnas a ocupar dependiendo de la importancia que tenga. Se omiten algunas columnas tales como las que son para el ID, como el nombre que tuvo el incendio, o la columna que indica de dónde se obtuvo el incendio; pues no deberían de afectar a la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Todas las columnas\n",
    "columnas = list(pd.read_csv(WF_DATA_COLUMNS_PATH))\n",
    "\n",
    "# Columnas que se ocuparán en el análisis\n",
    "columnas_ocupadas = [\n",
    "    \"FIRE_YEAR\",        # Año en que el incendio fue descubierto o confirmado\n",
    "    \"DISCOVERY_DATE\",   # Fecha en que fue descubiertoel incendio\n",
    "    \"DISCOVERY_TIME\",   # Hora en que fue descubierto el incendio\n",
    "    \"STAT_CAUSE_DESCR\", # Descripción de la causa del incendio\n",
    "    \"CONT_DATE\",        # Fecha en que se contuvo el incendio\n",
    "    \"CONT_TIME\",        # Hora en que se contuvo el incendio\n",
    "    \"FIRE_SIZE\",        # Área final estimada que alcanzó el incendio\n",
    "    \"FIRE_SIZE_CLASS\",\n",
    "    \"LATITUDE\",         # Latitud de la localización del incendio\n",
    "    \"LONGITUDE\",        # Longitud de la localización del incendio\n",
    "    \"STATE\",\n",
    "]\n",
    "\n",
    "# Columnas que no se ocuparán\n",
    "columnas_sin_ocupar = [x for x in columnas if x not in columnas_ocupadas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str que se ocupará al cargar los datos\n",
    "columnas_str = \",\".join(columnas_ocupadas)\n",
    "\n",
    "# Obtener una fila del dataset\n",
    "conn = crear_puntero(msg=True)\n",
    "df = pd.read_sql_query(f\"SELECT {columnas_str} FROM 'Fires'\", conn)\n",
    "conn.close()\n",
    "\n",
    "del columnas_str, conn\n",
    "\n",
    "print_cantidad(df)\n",
    "\n",
    "# df = df[df[\"FIRE_YEAR\"].map(int) >= 2011].reset_index()\n",
    "\n",
    "print_cantidad(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción preliminar de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos la información actual del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que 6 variables son del tipo numérico, y 5 son strings. Sin embargo, `DISCOVERY_DATE` y `CONT_DATE` son fechas. Esto se tratará más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos si no tiene datos duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dup = sum(df.duplicated())\n",
    "print(\"Existen\", n_dup, \"valores duplicados\")\n",
    "del n_dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que existen valores duplicados en el dataset, los dropeamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero observamos el número de elementos nulos que tiene el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado al gran tamaño de la base de datos, nos podemos dar \"el lujo\" de eliminar las filas que contengan elementos nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se quitan los valores nulos\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Se resetean los índices\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Se imprime la cantidad\n",
    "print_cantidad(df)\n",
    "\n",
    "# Y se muestran la cantidad de nulos que tiene actualmente\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora revisamos el tipo de datos que tiene el Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminamos con un dataset con la mitad de datos de cómo fue originalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de las variables de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de la columnas `{}_DATE_TIME`, a partir de las dos anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notemos que `{DISCOVERY,CONT}_DATE` está en formato timestamp, y que `{DISCOVERY,CONT}_TIME` está en formato `'hhmm'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_aux = [\"DISCOVERY_DATE\", \"DISCOVERY_TIME\", \n",
    "            \"CONT_DATE\", \"CONT_TIME\"]\n",
    "\n",
    "df[cols_aux].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una nueva columna que tenga la fecha y hora contenida. Pues la fecha está en formato Timestamp y la hora está en formato `'hhmm'`. Lo hacemos para los prefijos `'DISCOVERY_'` y `'CONT_'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generar_datetime(fecha_col, hora_col):\n",
    "    \"\"\"Recibe un DataFrame 'df', una columna 'fecha_col' que tiene \n",
    "    la fecha en formato de marca de tiempo (Timestamp) y una columna \n",
    "    'hora_col' que tiene la hora en formato 'hhmm'.\n",
    "\n",
    "    Retorna una columna del tipo 'Series', de datos del tipo datetime,\n",
    "    con la fecha y hora juntas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Transformamos la columna de fechas a la fecha \"usual\"\n",
    "    fecha_col_ = pd.to_datetime(fecha_col - pd.Timestamp(0).to_julian_date(),\n",
    "                                unit='D',\n",
    "                                dayfirst=True)\n",
    "    \n",
    "    # Obtenemos solo la parte de la fecha como una columna de str\n",
    "    fecha_col_ = fecha_col_.map(lambda x: str(x).split()[0])\n",
    "\n",
    "    # Procesamos la columna de la hora para que esté en formato \"hh:mm:ss\"\n",
    "    hora_col_ = hora_col.map(lambda x: \":\".join([\" \" + x[0:2], x[2:4], \"00\"]))\n",
    "\n",
    "    # Retornamos la suma de ambas, en formato de datetime\n",
    "    return pd.to_datetime(fecha_col_ + hora_col_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuevas columnas\n",
    "df[\"DISC_DATE_TIME\"] = generar_datetime(df[\"DISCOVERY_DATE\"],\n",
    "                                        df[\"DISCOVERY_TIME\"])\n",
    "df[\"CONT_DATE_TIME\"] = generar_datetime(df[\"CONT_DATE\"],\n",
    "                                        df[\"CONT_TIME\"])\n",
    "\n",
    "# Agregamos estas nuevas columnas al conjunto de columnas\n",
    "nuevas_cols = [\"DISC_DATE_TIME\", \"CONT_DATE_TIME\"]\n",
    "columnas += nuevas_cols\n",
    "columnas_ocupadas += nuevas_cols\n",
    "\n",
    "df[nuevas_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitamos ahora las columnas que resultan redundantes, al tener la misma información de los que tiene prefijo `_DATE_TIME`: \n",
    "* `DISCOVERY_DATE`, `DISCOVERY_TIME`.\n",
    "* `CONT_DATE`, `CONT_TIME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_aux:\n",
    "    columnas_ocupadas.remove(x)\n",
    "    columnas_sin_ocupar.append(x)\n",
    "\n",
    "del cols_aux, nuevas_cols\n",
    "    \n",
    "df = df[columnas_ocupadas]\n",
    "print_cantidad(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de las columnas `{}_MONTH` y `{}_DAY_OF_WEEK`, a partir de `{}_DATE_TIME`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos una columna de los meses y semanas, para ver si nos entrega información extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def number_to_month(n):\n",
    "    \"\"\"Hace un mapeo del número de un mes, al nombre del mes.\n",
    "    Retorna None si no se entrega un número entero entre el 1 y el 12.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = {\n",
    "        1:\"January\", 2:\"February\", 3:\"March\", 4:\"April\",\n",
    "        5:\"May\", 6:\"June\", 7:\"July\", 8:\"August\",\n",
    "        9:\"September\", 10:\"Octuber\", 11:\"November\", 12:\"December\"\n",
    "    }\n",
    "    \n",
    "    return m.get(n, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos las columnas {DISC,CONT}_{MONTH,DAY_OF_WEEK}\n",
    "for x in [\"DISC\", \"CONT\"]:\n",
    "    df[f\"{x}_MONTH\"] = df[f\"{x}_DATE_TIME\"].dt.month.map(number_to_month)\n",
    "    df[f\"{x}_DOW\"] = df[f\"{x}_DATE_TIME\"].dt.day_name()\n",
    "    df[f\"{x}_TIME\"] = df[f\"{x}_DATE_TIME\"].dt.hour.map(int)\n",
    "\n",
    "# Añadimos las nuevas columnas al conjunto de columnas\n",
    "cdt = columnas_ocupadas.pop(-1)\n",
    "columnas.pop(-1)\n",
    "cols_aux = [\"DISC_MONTH\", \"DISC_DOW\", \"DISC_TIME\",\n",
    "            cdt, \"CONT_MONTH\", \"CONT_DOW\", \"CONT_TIME\"]\n",
    "\n",
    "columnas.extend(cols_aux)\n",
    "columnas_ocupadas.extend(cols_aux)\n",
    "del cdt, cols_aux\n",
    "\n",
    "# Actualizamos el df\n",
    "df = df[columnas_ocupadas]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtener las diferencias de tiempo del control del incendio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DT_FIRE\"] = df[\"CONT_DATE_TIME\"] - df[\"DISC_DATE_TIME\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DT_FIRE\"] = df.DT_FIRE.map(lambda x: x.total_seconds()/3600)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orden por fecha de descubrimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"DISC_DATE_TIME\"], ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar DataFrame\n",
    "\n",
    "Salvamos el DataFrame en la carpeta respectiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DF:\n",
    "    df.to_csv(path.join(WF_FOLDER_PATH, \"WILDFIRES_USA.csv\"), index=False)\n",
    "    print(\"Dataset guardado.\")\n",
    "    s = \",\".join(list(df.columns))\n",
    "    pd.DataFrame(data={s:[0]}).to_csv(path.join(WF_FOLDER_PATH, \n",
    "                                                \"WILDFIRES_USA_COLUMNS.csv\"), \n",
    "                                      index=False)\n",
    "    print(\"Columnas del dataset guardadas.\")\n",
    "    del s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 216.333666,
   "position": {
    "height": "237.667px",
    "left": "824px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
