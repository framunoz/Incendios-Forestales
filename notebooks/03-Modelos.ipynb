{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Predicción\n",
    "##### Futuras mejoras\n",
    "* Estudiar bien los métodos de clasificación\n",
    "* Cambiar el KNearestNeigh.. pues no está bien implementado: Falta normalizar, definir métrica, etc.\n",
    "* Ordenar por tiempo y separar\n",
    "* hacer crossvalidation\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rutas usuales que se ocuparán en el notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Salvar gráficos\n",
    "save_graf = False\n",
    "\n",
    "# Variable que comenta el estado actual de las funciones\n",
    "MOSTRAR_INFO = False\n",
    "\n",
    "# RUTAS\n",
    "MAIN_PATH = path.join(\"..\")\n",
    "\n",
    "IMG_PATH = path.join(MAIN_PATH, \"imagenes\")\n",
    "DATA_PATH = path.join(MAIN_PATH, \"data\")\n",
    "INF_PATH = path.join(MAIN_PATH, \"informe\")\n",
    "\n",
    "TAB_PATH = path.join(INF_PATH, \"tablas\")\n",
    "\n",
    "WF_FOLDER_PATH = path.join(DATA_PATH,\n",
    "                           \"wildfires_us\")\n",
    "WF_DATA_PATH = path.join(WF_FOLDER_PATH,\n",
    "                         \"WILDFIRES_USA.csv\")\n",
    "WF_DATA_COLUMNS_PATH = path.join(WF_FOLDER_PATH,\n",
    "                                 \"WILDFIRES_USA_COLUMNS.csv\")\n",
    "\n",
    "# Tamaño de la imagen\n",
    "my_figsize = (10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que se ocupará para imprimir la información (número de filas y columnas) de un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def print_cantidad(dataframe):\n",
    "    \"\"\"Imprime la cantidad de datos que tiene el Data Frame.\n",
    "    \"\"\"\n",
    "    msg_cantidad = \"El dataset tiene una cantidad de {} datos y {} variables.\"\n",
    "    print(msg_cantidad.format(dataframe.shape[0], dataframe.shape[1]))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columnas a ocupar\n",
    "\n",
    "Se escojen las columnas a ocupar dependiendo de la importancia que tenga. Se omiten algunas columnas tales como las que son para el ID, como el nombre que tuvo el incendio, o la columna que indica de dónde se obtuvo el incendio; pues no deberían de afectar a la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Todas las columnas\n",
    "columnas = str(pd.read_csv(WF_DATA_COLUMNS_PATH).columns[0]).split(\",\")\n",
    "\n",
    "# Columnas que se ocuparán en el análisis\n",
    "columnas_ocupadas = columnas.copy()\n",
    "\n",
    "# Columnas que no se ocuparán\n",
    "columnas_sin_ocupar = [x for x in columnas if x not in columnas_ocupadas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(WF_DATA_PATH)\n",
    "\n",
    "# Convertimos los datos que sean fechas en ese tipo de dato\n",
    "df[\"DISC_DATE_TIME\"] = pd.to_datetime(df[\"DISC_DATE_TIME\"])\n",
    "df[\"CONT_DATE_TIME\"] = pd.to_datetime(df[\"CONT_DATE_TIME\"])\n",
    "\n",
    "print_cantidad(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de las tablas para los distintos escenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia del dataset\n",
    "df_1 = df.copy()\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df.copy()\n",
    "df_2 = df_2[ df_2[\"FIRE_YEAR\"] >= 2011 ]\n",
    "df_2.reset_index(drop=True, inplace=True)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def new_category(cause):\n",
    "    \"\"\"Re-categoriza las causas que se tenían originalmente.\n",
    "    \"\"\"\n",
    "    d_causes = {\n",
    "        \"Other\": ['Miscellaneous',\n",
    "                  'Missing/Undefined'],\n",
    "        \"Human\": ['Children',\n",
    "                  'Smoking',\n",
    "                  'Equipment Use',\n",
    "                  'Debris Burning',\n",
    "                  'Campfire',\n",
    "                  'Campfire',\n",
    "                  'Railroad',\n",
    "                  'Powerline',\n",
    "                  'Fireworks',\n",
    "                  'Structure'],\n",
    "        \"Natural\": ['Lightning'],\n",
    "        \"Malicious\": ['Arson']\n",
    "    }\n",
    "\n",
    "    for new_cause in d_causes:\n",
    "        if cause in d_causes[new_cause]:\n",
    "            return new_cause\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2.copy()\n",
    "df_3[\"STAT_CAUSE_DESCR\"] = df_3.STAT_CAUSE_DESCR.map(new_category)\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir variables categóricas a númericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causas = df[\"STAT_CAUSE_DESCR\"].unique().tolist()\n",
    "causas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas que no se ocuparán\n",
    "drop_list = [\"DISC_DATE_TIME\",\n",
    "             \"CONT_DATE_TIME\",\n",
    "             \"FIRE_SIZE_CLASS\", \n",
    "             \"FIRE_YEAR\", \n",
    "             \"CONT_MONTH\",\n",
    "             \"CONT_DOW\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas que no se ocuparán\n",
    "for to_drop in drop_list:\n",
    "    df_1 = df_1.drop(to_drop, axis=1)\n",
    "    df_2 = df_2.drop(to_drop, axis=1)\n",
    "    df_3 = df_3.drop(to_drop, axis=1)\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"STATE\", \n",
    "    \"DISC_MONTH\",\n",
    "    \"DISC_DOW\",\n",
    "    \"STAT_CAUSE_DESCR\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_list = [preprocessing.LabelEncoder() for _ in range(3)]\n",
    "df_list = [df_1, df_2, df_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    for i in range(3):\n",
    "        df_ = df_list[i]\n",
    "        le_ = le_list[i]\n",
    "        df_[col] = le_.fit_transform(df_[col])\n",
    "#     df_[col] = le.fit_transform(df_[col])\n",
    "    \n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de funciones auxiliares para los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def save_classification_report(y_true, y_pred,\n",
    "                               target_names=None,\n",
    "                               buf=None, label=None, caption=None,\n",
    "                               support=True, verbose=False, escenario_name=None,):\n",
    "    \"\"\"Salva el reporte en formato LaTeX.\n",
    "    \"\"\"\n",
    "    # Importaciones básicas\n",
    "    from sklearn.metrics import classification_report\n",
    "    import pandas as pd\n",
    "    from numpy import nan as NA\n",
    "    from os import path\n",
    "    \n",
    "    label = label if label is None else f\"tab:{label}\"\n",
    "    \n",
    "    if verbose: print(\"Creando Reporte...\")\n",
    "    # Guardamos el reporte de clasificación en formato de diccionario\n",
    "    d = classification_report(y_true, y_pred,\n",
    "                              target_names=target_names,\n",
    "                              output_dict=True)\n",
    "    \n",
    "    # Creamos una función que pasa el reporte a DataFrame\n",
    "    def dicc2df(d):\n",
    "        d = d.copy()\n",
    "        d['accuracy'] = {'precision': NA,\n",
    "                         'recall': NA,\n",
    "                         'f1-score': d['accuracy'],\n",
    "                         'support': d['weighted avg']['support']}\n",
    "        df = pd.DataFrame(d)\n",
    "        df = df.T\n",
    "        df[\"support\"] = df[\"support\"].astype('int32')\n",
    "        return df\n",
    "    \n",
    "    df = dicc2df(d)\n",
    "    \n",
    "    # Si no se quiere que se entregue el soporte\n",
    "    if not support: df.pop(\"support\")\n",
    "    if verbose: print(\"Reporte Creado!\")\n",
    "    \n",
    "    if verbose: print(\"Creando LaTeX...\")\n",
    "    # Guardamos el dataframe\n",
    "    df_latex = df.to_latex(buf=buf,\n",
    "                           na_rep=\"\",\n",
    "                           float_format=\"%.3f\",\n",
    "                           caption=caption,\n",
    "                           label=label,)\n",
    "    if verbose: print(\"LaTeX creado!\")\n",
    "    \n",
    "    if df_latex is not None and verbose:\n",
    "        print(\"El reporte en latex es el siguiente:\\n\")\n",
    "        print(df_latex)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crearan distintas funciones para predecir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def modelo_naive_bayes(X_tr, y_tr, X_te, y_te,\n",
    "                       verbose=MOSTRAR_INFO, target_names=None,\n",
    "                       escenario_name=None,\n",
    "                      ):\n",
    "    \"\"\"Genera el modelo de Naïve Bayes e imprime el resumen de clasificación\n",
    "    \"\"\"\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    \n",
    "    if verbose: print(\"Creando modelo...\")\n",
    "    model = GaussianNB()\n",
    "    \n",
    "    if verbose: print(\"Ajustando modelo...\")\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    if verbose: print(\"Prediciendo...\")\n",
    "    y_pred = model.predict(X_te)\n",
    "    \n",
    "    if verbose: print(classification_report(y_te, y_pred, target_names=target_names))\n",
    "    score = model.score(X_te, y_te)\n",
    "    \n",
    "    save_classification_report(y_te, y_pred,\n",
    "                               support=False, verbose=verbose,\n",
    "                               buf=path.join(TAB_PATH, \n",
    "                                             f\"NB_{escenario_name}.tex\"),\n",
    "                               caption=(\"Reporte de clasificación\"\n",
    "                                      f\" para el {escenario_name}\"\n",
    "                                      \" utilizando Naïve Bayes\"),\n",
    "                               label=f\"NB_{escenario_name}\",\n",
    "                               target_names=target_names\n",
    "                              )\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo K-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza una función auxiliar para encontrar el $k$ óptimo. Si el $k$ óptimo ya se encontró, devuelve ese valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "k_predicho, k_optimo = True, 40\n",
    "\n",
    "def buscar_k(X_tr, y_tr, X_te, y_te,\n",
    "             k_range=None, k_min=1, k_max=20, gap=1, \n",
    "             verbose=MOSTRAR_INFO):\n",
    "    \"\"\"Función utiliazada para determinar el k óptimo.\n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    global k_predicho\n",
    "    global k_optimo\n",
    "    \n",
    "    # Si ya había un k óptimo, retorna el valor de k\n",
    "    if k_predicho:\n",
    "        if verbose: print(f\"El k óptimo es:{k_optimo}\")\n",
    "        return k_optimo\n",
    "    \n",
    "    if verbose: print(\"Normalizando...\")\n",
    "    # Escalamos\n",
    "    scaler = MinMaxScaler()\n",
    "    X_tr_scaled = (scaler.fit_transform(X_tr))\n",
    "    X_te_scaled = (scaler.transform(X_te))\n",
    "    \n",
    "    if verbose: print(\"Buscando k óptimo...\")\n",
    "    # Vemos los distintos valores de k\n",
    "    k_range = range(k_min, k_max+1, gap) if not k_range else k_range\n",
    "    k_range = list(k_range)\n",
    "    scores = dict()\n",
    "    for k in k_range:\n",
    "        if verbose: print(f\"Entrenando con k={k}\")\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_tr_scaled, y_tr)\n",
    "        score = knn.score(X_te_scaled, y_te)\n",
    "        scores[k] = score\n",
    "#         if i > 0 and scores[k_range[i-1]] < scores[k_range[i]]:\n",
    "#             break\n",
    "        if verbose: print(f\"Score para k={k}:{score}\\n\" + \"=\"*40)\n",
    "    \n",
    "    k_optimo = max(k_range, key=lambda x: scores[x])\n",
    "    k_predicho = True\n",
    "    \n",
    "    if verbose: print(f\"El k óptimo es:{k_optimo}\")\n",
    "    \n",
    "    return k_optimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def modelo_knn(X_tr, y_tr, X_te, y_te, \n",
    "               verbose=MOSTRAR_INFO, target_names=None,\n",
    "               normalizar=True, buscar_k_optimo=not k_predicho, escenario_name=None,):\n",
    "    \"\"\"Genera el modelo de Naïve Bayes e imprime el resumen de clasificación.\n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    global k_predicho\n",
    "    global k_optimo\n",
    "    \n",
    "    if verbose and buscar_k_optimo: print(\"Buscando k óptimo...\")\n",
    "    if buscar_k_optimo:\n",
    "        k_predicho = False\n",
    "        k_optimo = None\n",
    "    k = buscar_k(X_tr, y_tr, X_te, y_te, verbose=False)\n",
    "    \n",
    "    if verbose and normalizar: print(\"Normalizando datos...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    X_tr = (scaler.fit_transform(X_tr)) if normalizar else X_tr\n",
    "    X_te = (scaler.transform(X_te)) if normalizar else X_te\n",
    "    \n",
    "    if verbose: print(\"Creando modelo...\")\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    if verbose: print(\"Ajustando modelo...\")\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    if verbose: print(\"Prediciendo...\")\n",
    "    y_pred = model.predict(X_te)\n",
    "    \n",
    "    if verbose: print(classification_report(y_te, y_pred, target_names=target_names))\n",
    "    score = model.score(X_te, y_te)\n",
    "    \n",
    "    save_classification_report(y_te, y_pred,\n",
    "                               support=False, verbose=verbose,\n",
    "                               buf=path.join(TAB_PATH, \n",
    "                                             f\"KNN_{escenario_name}.tex\"),\n",
    "                               caption=(\"Reporte de clasificación\"\n",
    "                                      f\" para el {escenario_name}\"\n",
    "                                      \" utilizando KNN\"),\n",
    "                               label=f\"KNN_{escenario_name}\",\n",
    "                               target_names=target_names\n",
    "                              )\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Discriminante Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def modelo_discr_lineal(X_tr, y_tr, X_te, y_te,\n",
    "                        verbose=MOSTRAR_INFO, target_names=None, escenario_name=None,):\n",
    "    \"\"\"Genera el modelo de Discriminante Lineal e imprime el resumen de clasificación\n",
    "    \"\"\"\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    \n",
    "    if verbose: print(\"Creando modelo...\")\n",
    "    model = LinearDiscriminantAnalysis()\n",
    "    \n",
    "    if verbose: print(\"Ajustando modelo...\")\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    if verbose: print(\"Prediciendo...\")\n",
    "    y_pred = model.predict(X_te)\n",
    "    \n",
    "    if verbose: print(classification_report(y_te, y_pred, target_names=target_names))\n",
    "    score = model.score(X_te, y_te)\n",
    "    \n",
    "    save_classification_report(y_te, y_pred,\n",
    "                               support=False, verbose=verbose,\n",
    "                               buf=path.join(TAB_PATH, \n",
    "                                             f\"DL_{escenario_name}.tex\"),\n",
    "                               caption=(\"Reporte de clasificación\"\n",
    "                                      f\" para el {escenario_name}\"\n",
    "                                      \" utilizando Discriminante Lineal\"),\n",
    "                               label=f\"DL_{escenario_name}\",\n",
    "                               target_names=target_names\n",
    "                              )\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Discriminante Cuadrático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def modelo_discr_cuadr(X_tr, y_tr, X_te, y_te,\n",
    "                       verbose=MOSTRAR_INFO, target_names=None, escenario_name=None,):\n",
    "    \"\"\"Genera el modelo de Discriminante Cuadrático e imprime el resumen de clasificación\n",
    "    \"\"\"\n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "    \n",
    "    if verbose: print(\"Creando modelo...\")\n",
    "    model = QuadraticDiscriminantAnalysis()\n",
    "    \n",
    "    if verbose: print(\"Ajustando modelo...\")\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    if verbose: print(\"Prediciendo...\")\n",
    "    y_pred = model.predict(X_te)\n",
    "    \n",
    "    if verbose: print(classification_report(y_te, y_pred, target_names=target_names))\n",
    "    score = model.score(X_te, y_te)\n",
    "    \n",
    "    save_classification_report(y_te, y_pred,\n",
    "                               support=False, verbose=verbose,\n",
    "                               buf=path.join(TAB_PATH, \n",
    "                                             f\"DC_{escenario_name}.tex\"),\n",
    "                               caption=(\"Reporte de clasificación\"\n",
    "                                      f\" para el {escenario_name}\"\n",
    "                                      \" utilizando Discriminante Cuadrático\"),\n",
    "                               label=f\"DC_{escenario_name}\",\n",
    "                               target_names=target_names\n",
    "                              )\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def modelo_random_forest(X_tr, y_tr, X_te, y_te,\n",
    "                         verbose=MOSTRAR_INFO, target_names=None, escenario_name=None,):\n",
    "    \"\"\"Genera el modelo de Random Forest e imprime el resumen de clasificación\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    if verbose: print(\"Creando modelo...\")\n",
    "    model = RandomForestClassifier(n_jobs=4,\n",
    "                                   random_state=3435)\n",
    "    \n",
    "    if verbose: print(\"Ajustando modelo...\")\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    if verbose: print(\"Prediciendo...\")\n",
    "    y_pred = model.predict(X_te)\n",
    "    \n",
    "    if verbose: print(classification_report(y_te, y_pred, target_names=target_names))\n",
    "    score = model.score(X_te, y_te)\n",
    "    \n",
    "    save_classification_report(y_te, y_pred,\n",
    "                               support=False, verbose=verbose,\n",
    "                               buf=path.join(TAB_PATH, \n",
    "                                             f\"RF_{escenario_name}.tex\"),\n",
    "                               caption=(\"Reporte de clasificación\"\n",
    "                                      f\" para el {escenario_name}\"\n",
    "                                      \" utilizando Random Forest\"),\n",
    "                               label=f\"RF_{escenario_name}\",\n",
    "                               target_names=target_names\n",
    "                              )\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "MODELOS = {\n",
    "    \"Discriminante_Cuadratico\": modelo_discr_cuadr,\n",
    "    \"Discriminante_Lineal\": modelo_discr_lineal,\n",
    "    \"KNN\": modelo_knn,\n",
    "    \"Naive_Bayes\": modelo_naive_bayes,\n",
    "    \"Random_Forest\": modelo_random_forest,\n",
    "}\n",
    "\n",
    "def resumen_modelos(X_tr, y_tr, X_te, y_te,\n",
    "                    verbose=MOSTRAR_INFO, target_names=None,\n",
    "                    escenario_name=None, save_table=False\n",
    "                   ):\n",
    "    \"\"\"Resume los modelos\n",
    "    \"\"\"\n",
    "    \n",
    "    data_ = (X_tr, y_tr, X_te, y_te)\n",
    "    \n",
    "    L_nombres = []\n",
    "    L_scores = []\n",
    "    for name in MODELOS:\n",
    "        if verbose: print(f\"Se está prediciendo el modelo {name}\")\n",
    "        L_nombres.append(name)\n",
    "        score = round(MODELOS[name](*data_, \n",
    "                                    verbose=verbose, \n",
    "                                    target_names=target_names,\n",
    "                                    escenario_name=escenario_name), 4)\n",
    "        L_scores.append(score)\n",
    "        if verbose: print(\"=\"*40)\n",
    "        \n",
    "        \n",
    "    d_scores = {\n",
    "        \"Nombres\": L_nombres,\n",
    "        \"Scores\": L_scores\n",
    "    }\n",
    "    \n",
    "    d_scores = pd.DataFrame(d_scores)\n",
    "    d_scores.set_index(\"Nombres\", inplace=True)\n",
    "    d_scores.sort_values(by=\"Scores\", inplace=True)\n",
    "    \n",
    "    return d_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer Escenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparación de la data de entrenamiento con la de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = df_1.drop(\"STAT_CAUSE_DESCR\", axis=1) # .values\n",
    "y_1 = df_1.STAT_CAUSE_DESCR # .values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names_1 = list(le_list[0].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1,\n",
    "                                                    y_1,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=0,\n",
    "                                                    shuffle=False\n",
    "                                                   )\n",
    "data_1 = (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba del primer escenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escenario_1 = resumen_modelos(*data_1,\n",
    "                              target_names=target_names_1,\n",
    "                              escenario_name=\"Escenario 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escenario_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo Escenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = df_2.drop(\"STAT_CAUSE_DESCR\", axis=1) # .values\n",
    "y_2 = df_2.STAT_CAUSE_DESCR # .values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names_2 = list(le_list[1].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_2,\n",
    "                                                    y_2,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=0,\n",
    "                                                    shuffle=False\n",
    "                                                   )\n",
    "data_2 = (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escenario_2 = resumen_modelos(*data_2,\n",
    "                              target_names=target_names_2,\n",
    "                              escenario_name=\"Escenario 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escenario_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tercer Escenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3 = df_3.drop(\"STAT_CAUSE_DESCR\", axis=1) # .values\n",
    "y_3 = df_3.STAT_CAUSE_DESCR # .values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names_3 = list(le_list[2].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_3,\n",
    "                                                    y_3,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=0,\n",
    "                                                    shuffle=False\n",
    "                                                   )\n",
    "data_3 = (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escenario_3 = resumen_modelos(*data_3,\n",
    "                              target_names=target_names_3,\n",
    "                              escenario_name=\"Escenario 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escenario_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escenario_1.rename(columns={\"Scores\":\"Escenario 1\"}, inplace=True)\n",
    "escenario_2.rename(columns={\"Scores\":\"Escenario 2\"}, inplace=True)\n",
    "escenario_3.rename(columns={\"Scores\":\"Escenario 3\"}, inplace=True)\n",
    "\n",
    "resumen = pd.concat([escenario_1, escenario_2, escenario_3],\n",
    "                    axis=1,\n",
    "                    sort=False)\n",
    "resumen.index.rename(\"Modelos\", inplace=True)\n",
    "\n",
    "resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumen[\"Nombres\"] = resumen.index\n",
    "resumen = resumen[[\"Nombres\", \"Experimento 1\", \"Experimento 2\", \"Experimento 3\"]]\n",
    "resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumen_save_name = path.join(TAB_PATH, \"resumen_experimentos.tex\")\n",
    "res = resumen.to_latex(index=True,\n",
    "                       buf=resumen_save_name,\n",
    "                       caption=\"Tabla resumen de los modelos\",\n",
    "                       label=\"tab:resumen-exp\",\n",
    "                       column_format=\"rccc\"\n",
    "                       )\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# # Entrenamos\n",
    "# rfc = RandomForestClassifier(n_estimators=100, class_weight= \"balanced\", verbose=1, max_depth=5) # class_weight=\"balanced_subsample\"\n",
    "# rfc = rfc.fit(X_train, y_train)\n",
    "\n",
    "# # Predecimos\n",
    "# y_pred_rfc = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "\n",
    "# multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = (X_train, y_train, X_test, y_test)\n",
    "# modelo_discr_cuadr(*data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# # Entrenamos\n",
    "# rfc = RandomForestClassifier(n_estimators=100, class_weight= \"balanced\", verbose=1, max_depth=5) # class_weight=\"balanced_subsample\"\n",
    "# rfc = rfc.fit(X_train, y_train)\n",
    "\n",
    "# # Predecimos\n",
    "# y_pred_rfc = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mostramos\n",
    "# print(classification_report(y_test, y_pred_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Entrenamos\n",
    "# rfc = RandomForestClassifier(n_estimators=50) # class_weight=\"balanced_subsample\"\n",
    "# rfc = rfc.fit(X_train, y_train)\n",
    "\n",
    "# # Predecimos\n",
    "# y_pred_rfc = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mostramos\n",
    "# print(classification_report(y_test, y_pred_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "228.667px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 216.333666,
   "position": {
    "height": "237.667px",
    "left": "824px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
